{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887116b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install -q albumentations\n",
    "!pip install -q kaggle\n",
    "!pip install -q plotly\n",
    "!pip install -q seaborn\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q opencv-python\n",
    "!pip install -q tf2onnx\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Core ML libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2B1\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Configure TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "from tensorflow.keras.mixed_precision import Policy\n",
    "policy = Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(\"‚úÖ Mixed precision enabled\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb87a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"/content/plantdisease.zip\").exists():\n",
    "    !kaggle datasets download -d emmarex/plantdisease -p /content\n",
    "\n",
    "!unzip -oq /content/plantdisease.zip -d /content/\n",
    "\n",
    "data_dir = Path('/content/PlantVillage') \n",
    "if data_dir.exists() and any(data_dir.iterdir()):\n",
    "    print(\"‚úÖ Dataset extracted successfully!\")\n",
    "    print(f\"Dataset path: {data_dir}\")\n",
    "\n",
    "    subdirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "    print(f\"\\nDataset contains {len(subdirs)} subdirectories:\")\n",
    "    for subdir in subdirs[:5]:\n",
    "        num_images = len(list(subdir.glob('*.jpg')) + list(subdir.glob('*.JPG')))\n",
    "        print(f\"  üìÅ {subdir.name}: {num_images} images\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... and {len(subdirs) - 5} more directories\")\n",
    "else:\n",
    "    print(\"‚ùå Extraction failed. Please check the .zip file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Dataset Exploration and Analysis\n",
    "print(\"üìä Analyzing dataset structure and class distribution...\")\n",
    "\n",
    "# Define dataset paths\n",
    "DATASET_DIR = Path('/content/plantdisease')\n",
    "IMG_SIZE = (224, 224)  # EfficientNetV2 recommended size\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 38\n",
    "\n",
    "# Get all class directories\n",
    "class_dirs = sorted([d for d in DATASET_DIR.iterdir() if d.is_dir()])\n",
    "class_names = [d.name for d in class_dirs]\n",
    "\n",
    "print(f\"üìã Found {len(class_names)} classes:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {i+1:2d}. {class_name}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = {}\n",
    "total_images = 0\n",
    "\n",
    "for class_dir in class_dirs:\n",
    "    # Count images (jpg, JPG, png, PNG)\n",
    "    image_files = (list(class_dir.glob('*.jpg')) + \n",
    "                  list(class_dir.glob('*.JPG')) + \n",
    "                  list(class_dir.glob('*.png')) + \n",
    "                  list(class_dir.glob('*.PNG')))\n",
    "    \n",
    "    count = len(image_files)\n",
    "    class_counts[class_dir.name] = count\n",
    "    total_images += count\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"Total Images: {total_images:,}\")\n",
    "print(f\"Total Classes: {len(class_names)}\")\n",
    "print(f\"Average per class: {total_images // len(class_names):,}\")\n",
    "\n",
    "# Find min/max class sizes\n",
    "min_class = min(class_counts, key=class_counts.get)\n",
    "max_class = max(class_counts, key=class_counts.get)\n",
    "print(f\"Smallest class: {min_class} ({class_counts[min_class]} images)\")\n",
    "print(f\"Largest class: {max_class} ({class_counts[max_class]} images)\")\n",
    "\n",
    "# Create class distribution visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "classes_sorted = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "classes, counts = zip(*classes_sorted)\n",
    "\n",
    "plt.bar(range(len(classes)), counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "plt.title('Class Distribution in PlantDisease Dataset', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Plant Disease Classes', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + 50, str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate class balance\n",
    "class_balance = max(counts) / min(counts)\n",
    "print(f\"\\n‚öñÔ∏è Class Balance Ratio: {class_balance:.2f}:1\")\n",
    "if class_balance > 3:\n",
    "    print(\"‚ö†Ô∏è  Dataset is imbalanced. We'll use class weights and balanced sampling.\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset is reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Sample Images Visualization\n",
    "print(\"üñºÔ∏è Displaying sample images from each class...\")\n",
    "\n",
    "# Create a grid of sample images\n",
    "fig, axes = plt.subplots(6, 6, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show samples from first 36 classes\n",
    "for idx, class_dir in enumerate(class_dirs[:36]):\n",
    "    # Get a random image from this class\n",
    "    image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "    if image_files:\n",
    "        random_image = random.choice(image_files)\n",
    "        \n",
    "        # Load and display image\n",
    "        img = Image.open(random_image)\n",
    "        img = img.resize((150, 150))\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(class_dir.name, fontsize=10, pad=10)\n",
    "        axes[idx].axis('off')\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, 'No images', ha='center', va='center')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Plant Disease Dataset', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display image size analysis\n",
    "print(\"\\nüìè Analyzing image dimensions...\")\n",
    "sample_sizes = []\n",
    "for class_dir in class_dirs[:5]:  # Sample from first 5 classes\n",
    "    image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "    for img_path in image_files[:10]:  # Sample 10 images per class\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                sample_sizes.append(img.size)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "if sample_sizes:\n",
    "    widths, heights = zip(*sample_sizes)\n",
    "    print(f\"Sample image dimensions:\")\n",
    "    print(f\"  Width range: {min(widths)} - {max(widths)} pixels\")\n",
    "    print(f\"  Height range: {min(heights)} - {max(heights)} pixels\")\n",
    "    print(f\"  Most common size: {max(set(sample_sizes), key=sample_sizes.count)}\")\n",
    "    print(f\"‚úÖ We'll resize all images to {IMG_SIZE} for consistent training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a249857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Preprocessing and Augmentation Pipeline\n",
    "class AdvancedDataGenerator:\n",
    "    \"\"\"Advanced data generator with custom augmentation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir, img_size=(224, 224), batch_size=32):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.class_names = sorted([d.name for d in self.dataset_dir.iterdir() if d.is_dir()])\n",
    "        self.num_classes = len(self.class_names)\n",
    "        \n",
    "        # Create class to index mapping\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.class_names)}\n",
    "        \n",
    "        # Calculate class weights for imbalanced dataset\n",
    "        self.class_weights = self._calculate_class_weights()\n",
    "        \n",
    "        # Define augmentation strategies\n",
    "        self.train_augmentation = A.Compose([\n",
    "            A.Resize(height=img_size[0], width=img_size[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Rotate(limit=15, p=0.3),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2, \n",
    "                contrast_limit=0.2, \n",
    "                p=0.3\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, \n",
    "                sat_shift_limit=20, \n",
    "                val_shift_limit=10, \n",
    "                p=0.3\n",
    "            ),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "            A.OneOf([\n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.ElasticTransform(p=0.5),\n",
    "            ], p=0.2),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.val_augmentation = A.Compose([\n",
    "            A.Resize(height=img_size[0], width=img_size[1]),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def _calculate_class_weights(self):\n",
    "        \"\"\"Calculate class weights to handle imbalanced dataset\"\"\"\n",
    "        class_counts = {}\n",
    "        for class_dir in self.dataset_dir.iterdir():\n",
    "            if class_dir.is_dir():\n",
    "                count = len(list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG')))\n",
    "                class_counts[class_dir.name] = count\n",
    "        \n",
    "        total_samples = sum(class_counts.values())\n",
    "        weights = {}\n",
    "        \n",
    "        for class_name, count in class_counts.items():\n",
    "            weight = total_samples / (self.num_classes * count)\n",
    "            weights[self.class_to_idx[class_name]] = weight\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def create_dataset_split(self, train_split=0.8, val_split=0.15, test_split=0.05):\n",
    "        \"\"\"Create train/validation/test splits\"\"\"\n",
    "        all_files = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for class_dir in self.dataset_dir.iterdir():\n",
    "            if class_dir.is_dir():\n",
    "                class_idx = self.class_to_idx[class_dir.name]\n",
    "                image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "                \n",
    "                for img_file in image_files:\n",
    "                    all_files.append(str(img_file))\n",
    "                    all_labels.append(class_idx)\n",
    "        \n",
    "        # Create stratified splits\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            all_files, all_labels, \n",
    "            test_size=test_split, \n",
    "            stratify=all_labels, \n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, \n",
    "            test_size=val_split/(train_split + val_split), \n",
    "            stratify=y_temp, \n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \n",
    "    def load_and_preprocess_image(self, image_path, augmentation):\n",
    "        \"\"\"Load and preprocess a single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(str(image_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply augmentation\n",
    "            augmented = augmentation(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "            return image.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            blank = np.zeros((*self.img_size, 3), dtype=np.float32)\n",
    "            return blank\n",
    "    \n",
    "    def create_tf_dataset(self, files, labels, augmentation, shuffle=True):\n",
    "        \"\"\"Create TensorFlow dataset\"\"\"\n",
    "        def load_image_fn(image_path, label):\n",
    "            # Convert tensor to string for file path\n",
    "            image_path_str = tf.py_function(lambda x: x.numpy().decode('utf-8'), [image_path], tf.string)\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            image = tf.py_function(\n",
    "                lambda path: self.load_and_preprocess_image(path.numpy().decode('utf-8'), augmentation),\n",
    "                [image_path_str],\n",
    "                tf.float32\n",
    "            )\n",
    "            image.set_shape([*self.img_size, 3])\n",
    "            \n",
    "            # One-hot encode label\n",
    "            label_onehot = tf.one_hot(label, self.num_classes)\n",
    "            \n",
    "            return image, label_onehot\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=1000, seed=SEED)\n",
    "        \n",
    "        dataset = dataset.map(load_image_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# Initialize data generator\n",
    "data_gen = AdvancedDataGenerator(DATASET_DIR, IMG_SIZE, BATCH_SIZE)\n",
    "\n",
    "print(f\"‚úÖ Data generator initialized\")\n",
    "print(f\"üìä Found {data_gen.num_classes} classes\")\n",
    "print(f\"üéØ Class weights calculated for balanced training\")\n",
    "\n",
    "# Create dataset splits\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data_gen.create_dataset_split()\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"  Training: {len(X_train):,} images\")\n",
    "print(f\"  Validation: {len(X_val):,} images\")\n",
    "print(f\"  Test: {len(X_test):,} images\")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = data_gen.create_tf_dataset(X_train, y_train, data_gen.train_augmentation, shuffle=True)\n",
    "val_dataset = data_gen.create_tf_dataset(X_val, y_val, data_gen.val_augmentation, shuffle=False)\n",
    "test_dataset = data_gen.create_tf_dataset(X_test, y_test, data_gen.val_augmentation, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ TensorFlow datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af13677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building advanced EfficientNetV2 model architecture\n",
    "\n",
    "class PlantDiseaseModel:\n",
    "    \"\"\"Advanced plant disease detection model with transfer learning\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, img_size=(224, 224), model_name='efficientnetv2-b0'):\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, dropout_rate=0.3, l2_reg=1e-4):\n",
    "        \"\"\"Build model with EfficientNetV2 backbone\"\"\"\n",
    "\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=(*self.img_size, 3), name='input_image')\n",
    "\n",
    "        # Base model (EfficientNetV2)\n",
    "        if self.model_name == 'efficientnetv2-b0':\n",
    "            base_model = EfficientNetV2B0(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_tensor=inputs,\n",
    "                pooling='avg' # Global average pooling\n",
    "            )\n",
    "        elif self.model_name == 'efficientnetv2-b1':\n",
    "            base_model = EfficientNetV2B1(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_tensor=inputs,\n",
    "                pooling='avg'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model name: {self.model_name}\")\n",
    "\n",
    "        # Freeze base model initially\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Base model output (already pooled)\n",
    "        x = base_model.output\n",
    "\n",
    "        # Custom classification head\n",
    "        x = layers.BatchNormalization(name='bn_1')(x)\n",
    "        x = layers.Dropout(dropout_rate, name='dropout_1')(x)\n",
    "\n",
    "        x1 = layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                          name='dense_1')(x)\n",
    "        x1 = layers.BatchNormalization(name='bn_2')(x1)\n",
    "        x1 = layers.Dropout(dropout_rate/2, name='dropout_2')(x1)\n",
    "\n",
    "        x2 = layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                          name='dense_2')(x1)\n",
    "        x2 = layers.BatchNormalization(name='bn_3')(x2)\n",
    "        x2 = layers.Dropout(dropout_rate/2, name='dropout_3')(x2)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                               name='predictions')(x2)\n",
    "\n",
    "        # Final model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='PlantDiseaseDetector')\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def unfreeze_base_model(self, unfreeze_layers=-30):\n",
    "        \"\"\"Unfreeze top layers of base model for fine-tuning\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be built first\")\n",
    "\n",
    "        # Find EfficientNet backbone\n",
    "        base_model = None\n",
    "        for layer in self.model.layers:\n",
    "            if 'efficientnet' in layer.name.lower():\n",
    "                base_model = layer\n",
    "                break\n",
    "\n",
    "        if base_model:\n",
    "            base_model.trainable = True\n",
    "            if abs(unfreeze_layers) > len(base_model.layers):\n",
    "                print(f\"Warning: unfreeze_layers ({unfreeze_layers}) > total layers ({len(base_model.layers)}). Unfreezing all.\")\n",
    "                unfreeze_layers = -len(base_model.layers)\n",
    "\n",
    "            for layer in base_model.layers[:unfreeze_layers]:\n",
    "                layer.trainable = False\n",
    "\n",
    "            print(f\"‚úÖ Unfroze top {abs(unfreeze_layers)} layers of base model\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not find base EfficientNet layer in the model.\")\n",
    "\n",
    "    def compile_model(self, learning_rate=1e-3, label_smoothing=0.1):\n",
    "        \"\"\"Compile model with advanced optimization\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be built first\")\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "\n",
    "        metrics = [\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        print(\"‚úÖ Model compiled successfully\")\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model_builder = PlantDiseaseModel(num_classes=data_gen.num_classes, img_size=IMG_SIZE)\n",
    "model = model_builder.build_model(dropout_rate=0.3, l2_reg=1e-4)\n",
    "\n",
    "print(f\"üèóÔ∏è Model Architecture:\")\n",
    "print(f\"  Input Shape: {model.input_shape}\")\n",
    "print(f\"  Output Shape: {model.output_shape}\")\n",
    "print(f\"  Total Parameters: {model.count_params():,}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(w.shape) for w in model.trainable_weights])\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "# Compile model\n",
    "model_builder.compile_model(learning_rate=1e-3)\n",
    "\n",
    "# Display first & last layers\n",
    "print(\"\\nüîç Model Summary (Key Layers):\")\n",
    "for i, layer in enumerate(model.layers[:5]):\n",
    "    try:\n",
    "        if isinstance(layer, layers.InputLayer):\n",
    "            shape_info = model.input_shape\n",
    "        else:\n",
    "            shape_info = layer.output_shape\n",
    "    except AttributeError:\n",
    "        # Fallback: compute output shape dynamically\n",
    "        shape_info = layer.compute_output_shape(model.input_shape)\n",
    "    print(f\"  {i+1}. {layer.name}: {shape_info}\")\n",
    "\n",
    "print(\"  ...\")\n",
    "\n",
    "for i, layer in enumerate(model.layers[-5:], len(model.layers)-5):\n",
    "    try:\n",
    "        if isinstance(layer, layers.InputLayer):\n",
    "            shape_info = model.input_shape\n",
    "        else:\n",
    "            shape_info = layer.output_shape\n",
    "    except AttributeError:\n",
    "        shape_info = layer.compute_output_shape(model.input_shape)\n",
    "    print(f\"  {i+1}. {layer.name}: {shape_info}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Training Configuration and Callbacks\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages training process with advanced callbacks and monitoring\"\"\"\n",
    "\n",
    "    def __init__(self, model, model_builder, class_weights):\n",
    "        self.model = model\n",
    "        self.model_builder = model_builder\n",
    "        self.class_weights = class_weights\n",
    "        self.history = None\n",
    "\n",
    "    def get_callbacks(self, patience=7):\n",
    "        \"\"\"Get comprehensive training callbacks\"\"\"\n",
    "\n",
    "        callbacks_list = [\n",
    "            # Model checkpointing - save best model\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath='/content/best_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            ),\n",
    "\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1,\n",
    "                mode='max'\n",
    "            ),\n",
    "\n",
    "            # Learning rate reduction\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=patience//2,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1,\n",
    "                mode='min'\n",
    "            ),\n",
    "\n",
    "            # Cosine annealing schedule\n",
    "            tf.keras.callbacks.LearningRateScheduler(\n",
    "                self._cosine_annealing_schedule,\n",
    "                verbose=0\n",
    "            ),\n",
    "\n",
    "            # CSV logger for training history\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                '/content/training_log.csv',\n",
    "                append=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return callbacks_list\n",
    "\n",
    "    def _cosine_annealing_schedule(self, epoch, lr):\n",
    "        \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "        max_epochs = 50\n",
    "        min_lr = 1e-7\n",
    "        max_lr = 1e-3\n",
    "\n",
    "        if epoch < 5:  # Warmup\n",
    "            return min_lr + (max_lr - min_lr) * epoch / 5\n",
    "        else:\n",
    "            return min_lr + (max_lr - min_lr) * (1 + np.cos(np.pi * epoch / max_epochs)) / 2\n",
    "\n",
    "    def train_phase1(self, train_dataset, val_dataset, epochs=20):\n",
    "        \"\"\"Phase 1: Train with frozen base model\"\"\"\n",
    "        print(\"üöÄ Phase 1: Training with frozen base model...\")\n",
    "\n",
    "        callbacks = self.get_callbacks(patience=5)\n",
    "\n",
    "        self.history_phase1 = self.model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=self.class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Phase 1 training completed\")\n",
    "        return self.history_phase1\n",
    "\n",
    "    def train_phase2(self, train_dataset, val_dataset, epochs=30, fine_tune_lr=1e-5):\n",
    "        \"\"\"Phase 2: Fine-tuning with unfrozen base model\"\"\"\n",
    "        print(\"üîß Phase 2: Fine-tuning with unfrozen base model...\")\n",
    "\n",
    "        # Unfreeze base model\n",
    "        self.model_builder.unfreeze_base_model(unfreeze_layers=-30)\n",
    "\n",
    "        # Recompile with lower learning rate\n",
    "        self.model_builder.compile_model(learning_rate=fine_tune_lr)\n",
    "\n",
    "        callbacks = self.get_callbacks(patience=10)\n",
    "\n",
    "        self.history_phase2 = self.model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=self.class_weights,\n",
    "            verbose=1\n",
    "        ) # Removed incorrect line continuation\n",
    "\n",
    "        print(\"‚úÖ Phase 2 fine-tuning completed\")\n",
    "        return self.history_phase2\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot comprehensive training history\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Combine histories if both phases exist\n",
    "        if hasattr(self, 'history_phase1') and hasattr(self, 'history_phase2'):\n",
    "            combined_history = {}\n",
    "            for key in self.history_phase1.history.keys():\n",
    "                combined_history[key] = (self.history_phase1.history[key] +\n",
    "                                       self.history_phase2.history[key])\n",
    "            history = combined_history\n",
    "            phase1_epochs = len(self.history_phase1.history['loss'])\n",
    "        else:\n",
    "            history = self.history_phase1.history if hasattr(self, 'history_phase1') else {}\n",
    "            phase1_epochs = 0\n",
    "\n",
    "        epochs = range(1, len(history.get('loss', [])) + 1)\n",
    "\n",
    "        # Plot training & validation accuracy\n",
    "        axes[0, 0].plot(epochs, history.get('accuracy', []), 'b-', label='Training Accuracy')\n",
    "        axes[0, 0].plot(epochs, history.get('val_accuracy', []), 'r-', label='Validation Accuracy')\n",
    "        if phase1_epochs > 0:\n",
    "            axes[0, 0].axvline(x=phase1_epochs, color='green', linestyle='--', alpha=0.7, label='Fine-tuning Start')\n",
    "        axes[0, 0].set_title('Model Accuracy')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot training & validation loss\n",
    "        axes[0, 1].plot(epochs, history.get('loss', []), 'b-', label='Training Loss')\n",
    "        axes[0, 1].plot(epochs, history.get('val_loss', []), 'r-', label='Validation Loss')\n",
    "        if phase1_epochs > 0:\n",
    "            axes[0, 1].axvline(x=phase1_epochs, color='green', linestyle='--', alpha=0.7, label='Fine-tuning Start')\n",
    "        axes[0, 1].set_title('Model Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot learning rate\n",
    "        if 'lr' in history:\n",
    "            axes[1, 0].plot(epochs, history['lr'], 'g-', label='Learning Rate')\n",
    "            axes[1, 0].set_title('Learning Rate Schedule')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Learning Rate')\n",
    "            axes[1, 0].set_yscale('log')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot top-3 accuracy\n",
    "        if 'top3_accuracy' in history:\n",
    "            axes[1, 1].plot(epochs, history.get('top3_accuracy', []), 'purple', label='Training Top-3')\n",
    "            axes[1, 1].plot(epochs, history.get('val_top3_accuracy', []), 'orange', label='Validation Top-3')\n",
    "            axes[1, 1].set_title('Top-3 Accuracy')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Top-3 Accuracy')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print final metrics\n",
    "        if history:\n",
    "            final_acc = history.get('val_accuracy', [0])[-1]\n",
    "            final_loss = history.get('val_loss', [float('inf')])[-1]\n",
    "            final_top3 = history.get('val_top3_accuracy', [0])[-1]\n",
    "\n",
    "            print(f\"\\nüìä Final Training Results:\")\n",
    "            print(f\"  Validation Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "            print(f\"  Validation Loss: {final_loss:.4f}\")\n",
    "            print(f\"  Validation Top-3 Accuracy: {final_top3:.4f} ({final_top3*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Initialize training manager\n",
    "trainer = TrainingManager(model, model_builder, data_gen.class_weights)\n",
    "\n",
    "print(\"‚úÖ Training manager initialized\")\n",
    "print(\"üìã Training will proceed in 2 phases:\")\n",
    "print(\"  Phase 1: Frozen base model (20 epochs)\")\n",
    "print(\"  Phase 2: Fine-tuning (30 epochs)\")\n",
    "print(\"\\nüéØ Expected final accuracy: >95%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÉ‚Äç‚ôÇÔ∏è Model Training Execution\n",
    "\n",
    "# Phase 1: Train with frozen base model\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: TRANSFER LEARNING WITH FROZEN BASE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history_phase1 = trainer.train_phase1(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "# Display Phase 1 results\n",
    "trainer.plot_training_history()\n",
    "\n",
    "# Phase 2: Fine-tune with unfrozen base model\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: FINE-TUNING WITH UNFROZEN BASE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history_phase2 = trainer.train_phase2(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=30,\n",
    "    fine_tune_lr=1e-5\n",
    ")\n",
    "\n",
    "# Display final training results\n",
    "trainer.plot_training_history()\n",
    "\n",
    "print(\"\\\\nüéâ Training completed successfully!\")\n",
    "print(\"üìÅ Best model saved to: /content/best_model.h5\")\n",
    "print(\"üìä Training log saved to: /content/training_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Comprehensive Model Evaluation\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_dataset, class_names):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(class_names)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        print(\"üîç Evaluating model on test dataset...\")\n",
    "        \n",
    "        # Get predictions and true labels\n",
    "        y_pred_probs = []\n",
    "        y_true = []\n",
    "        \n",
    "        for batch_images, batch_labels in self.test_dataset:\n",
    "            batch_pred = self.model.predict(batch_images, verbose=0)\n",
    "            y_pred_probs.extend(batch_pred)\n",
    "            y_true.extend(batch_labels.numpy())\n",
    "        \n",
    "        y_pred_probs = np.array(y_pred_probs)\n",
    "        y_true = np.array(y_true)\n",
    "        \n",
    "        # Convert one-hot to class indices\n",
    "        y_true_idx = np.argmax(y_true, axis=1)\n",
    "        y_pred_idx = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        test_accuracy = np.mean(y_true_idx == y_pred_idx)\n",
    "        \n",
    "        # Calculate top-3 accuracy\n",
    "        top3_pred = np.argsort(y_pred_probs, axis=1)[:, -3:]\n",
    "        top3_accuracy = np.mean([true_label in pred_top3 for true_label, pred_top3 in zip(y_true_idx, top3_pred)])\n",
    "        \n",
    "        print(f\"\\nüìà Test Set Performance:\")\n",
    "        print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "        print(f\"  Top-3 Accuracy: {top3_accuracy:.4f} ({top3_accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return y_true_idx, y_pred_idx, y_pred_probs\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"Plot detailed confusion matrix\"\"\"        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(20, 16))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names,\n",
    "                    cbar_kws={'shrink': 0.8})\n",
    "        plt.title('Confusion Matrix - Plant Disease Detection', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show per-class performance\n",
    "        print(\"\\nüéØ Per-Class Performance:\")\n",
    "        class_performance = list(zip(self.class_names, class_accuracies))\n",
    "        class_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (class_name, accuracy) in enumerate(class_performance):\n",
    "            status = \"üü¢\" if accuracy > 0.9 else \"üü°\" if accuracy > 0.8 else \"üî¥\"\n",
    "            print(f\"  {status} {class_name:25s}: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    \n",
    "    def plot_classification_report(self, y_true, y_pred):\n",
    "        \"\"\"Generate detailed classification report\"\"\"        \n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        report = classification_report(y_true, y_pred, target_names=self.class_names, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        \n",
    "        # Plot precision, recall, f1-score\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 12))\n",
    "        metrics = ['precision', 'recall', 'f1-score']\n",
    "        colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "        \n",
    "        for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "            class_scores = report_df[metric][:-3]  # Exclude accuracy, macro avg, weighted avg\n",
    "            sorted_scores = class_scores.sort_values(ascending=True)\n",
    "            y_pos = np.arange(len(sorted_scores))\n",
    "            axes[i].barh(y_pos, sorted_scores.values, color=color, alpha=0.7, edgecolor='navy')\n",
    "            axes[i].set_yticks(y_pos)\n",
    "            axes[i].set_yticklabels(sorted_scores.index, fontsize=8)\n",
    "            axes[i].set_xlabel(metric.capitalize(), fontsize=12)\n",
    "            axes[i].set_title(f'{metric.capitalize()} by Class', fontsize=14, fontweight='bold')\n",
    "            axes[i].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            for j, v in enumerate(sorted_scores.values):\n",
    "                axes[i].text(v + 0.01, j, f'{v:.3f}', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìä Overall Performance Summary:\")\n",
    "        print(f\"  Macro Average Precision: {report['macro avg']['precision']:.4f}\")\n",
    "        print(f\"  Macro Average Recall: {report['macro avg']['recall']:.4f}\")\n",
    "        print(f\"  Macro Average F1-Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "        print(f\"  Weighted Average F1-Score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    def analyze_misclassifications(self, y_true, y_pred, y_pred_probs, top_n=10):\n",
    "        \"\"\"Analyze most common misclassifications\"\"\"        \n",
    "        misclassified = y_true != y_pred\n",
    "        misclassified_indices = np.where(misclassified)[0]\n",
    "        \n",
    "        print(f\"\\n‚ùå Misclassification Analysis:\")\n",
    "        print(f\"Total misclassified: {len(misclassified_indices)} out of {len(y_true)} ({len(misclassified_indices)/len(y_true)*100:.2f}%)\")\n",
    "        \n",
    "        misclass_patterns = {}\n",
    "        for idx in misclassified_indices:\n",
    "            true_class = self.class_names[y_true[idx]]\n",
    "            pred_class = self.class_names[y_pred[idx]]\n",
    "            pattern = f\"{true_class} -> {pred_class}\"\n",
    "            \n",
    "            if pattern not in misclass_patterns:\n",
    "                misclass_patterns[pattern] = []\n",
    "            misclass_patterns[pattern].append(idx)\n",
    "        \n",
    "        sorted_patterns = sorted(misclass_patterns.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        print(f\"\\nüîç Top {min(top_n, len(sorted_patterns))} Misclassification Patterns:\")\n",
    "        for i, (pattern, indices) in enumerate(sorted_patterns[:top_n]):\n",
    "            frequency = len(indices)\n",
    "            percentage = frequency / len(misclassified_indices) * 100\n",
    "            print(f\"  {i+1:2d}. {pattern:50s}: {frequency:3d} cases ({percentage:.1f}%)\")\n",
    "    \n",
    "    def plot_confidence_distribution(self, y_pred_probs):\n",
    "        \"\"\"Plot prediction confidence distribution\"\"\"        \n",
    "        max_confidences = np.max(y_pred_probs, axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Histogram of confidence scores\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(max_confidences, bins=50, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "        plt.axvline(np.mean(max_confidences), color='red', linestyle='--', label=f'Mean: {np.mean(max_confidences):.3f}')\n",
    "        plt.axvline(np.median(max_confidences), color='green', linestyle='--', label=f'Median: {np.median(max_confidences):.3f}')\n",
    "        plt.title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Box plot of confidence scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(max_confidences, labels=['All Predictions'])\n",
    "        plt.title('Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Confidence Score')\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Confidence Statistics:\")\n",
    "        print(f\"  Mean Confidence: {np.mean(max_confidences):.4f}\")\n",
    "        print(f\"  Median Confidence: {np.median(max_confidences):.4f}\")\n",
    "        print(f\"  Std Confidence: {np.std(max_confidences):.4f}\")\n",
    "        print(f\"  Low Confidence (<0.8): {np.sum(max_confidences < 0.8)} samples ({np.sum(max_confidences < 0.8)/len(max_confidences)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Perform comprehensive evaluation\n",
    "evaluator = ModelEvaluator(model, test_dataset, data_gen.class_names)\n",
    "y_true, y_pred, y_pred_probs = evaluator.evaluate_model()\n",
    "\n",
    "# Generate visualizations and analysis\n",
    "evaluator.plot_confusion_matrix(y_true, y_pred)\n",
    "evaluator.plot_classification_report(y_true, y_pred)\n",
    "evaluator.analyze_misclassifications(y_true, y_pred, y_pred_probs)\n",
    "evaluator.plot_confidence_distribution(y_pred_probs)\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import tf2onnx\n",
    "    TF2ONNX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF2ONNX_AVAILABLE = False\n",
    "\n",
    "\n",
    "class ModelExporter:\n",
    "    def __init__(self, model: tf.keras.Model, class_names: list, export_dir: str = \"exported_models\"):\n",
    "        self.model = model\n",
    "        self.class_names = class_names\n",
    "        # Use model.input_shape which is reliable\n",
    "        self.input_shape = model.input_shape[1:]\n",
    "        self.export_dir = Path(export_dir)\n",
    "        self.export_dir.mkdir(exist_ok=True)\n",
    "        print(f\"üì§ Initialized model exporter. Files will be saved to: {self.export_dir.resolve()}\")\n",
    "\n",
    "    def export_tensorflow_model(self):\n",
    "        print(\"üíæ Exporting TensorFlow SavedModel...\")\n",
    "        tf_model_path = self.export_dir / 'tensorflow_model'\n",
    "        try:\n",
    "            # Use model.export() for SavedModel format in Keras 3+\n",
    "            self.model.export(tf_model_path)\n",
    "            print(f\"‚úÖ TensorFlow model saved to: {tf_model_path}\")\n",
    "            return tf_model_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to export TensorFlow SavedModel: {e}\")\n",
    "            return None\n",
    "\n",
    "    def export_keras_model(self):\n",
    "        print(\"üíæ Exporting Keras models...\")\n",
    "        h5_path = self.export_dir / 'plant_disease_model.h5'\n",
    "        keras_path = self.export_dir / 'plant_disease_model.keras'\n",
    "        try:\n",
    "            # Save in both H5 (legacy) and new Keras format\n",
    "            self.model.save(h5_path)\n",
    "            self.model.save(keras_path)\n",
    "            print(\"‚úÖ Keras models saved:\")\n",
    "            print(f\"  H5 format: {h5_path}\")\n",
    "            print(f\"  Keras format: {keras_path}\")\n",
    "            return h5_path, keras_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to export Keras models: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def export_tflite_model(self):\n",
    "        print(\"üì± Exporting TensorFlow Lite model...\")\n",
    "        try:\n",
    "            tf_model_path = self.export_dir / 'tensorflow_model'\n",
    "            if not tf_model_path.exists():\n",
    "                print(\"TensorFlow SavedModel not found, attempting to create it for TFLite conversion.\")\n",
    "                saved_model_path = self.export_tensorflow_model()\n",
    "                if not saved_model_path:\n",
    "                    print(\"Skipping TFLite export as SavedModel creation failed.\")\n",
    "                    return None\n",
    "\n",
    "            converter = tf.lite.TFLiteConverter.from_saved_model(str(tf_model_path))\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            tflite_model = converter.convert()\n",
    "\n",
    "            tflite_path = self.export_dir / 'plant_disease_model.tflite'\n",
    "            tflite_path.write_bytes(tflite_model)\n",
    "\n",
    "            size_mb = len(tflite_model) / (1024 * 1024)\n",
    "            print(f\"‚úÖ TensorFlow Lite model saved: {tflite_path}\")\n",
    "            print(f\"   Model size: {size_mb:.2f} MB\")\n",
    "            return tflite_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to export TensorFlow Lite model: {e}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            return None\n",
    "\n",
    "    def export_onnx_model(self):\n",
    "        print(\"üîÑ Exporting ONNX model...\")\n",
    "        if not TF2ONNX_AVAILABLE:\n",
    "            print(\"‚ùå tf2onnx not installed. Skipping ONNX export.\")\n",
    "            print(\"   Install with: pip install tf2onnx\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            onnx_path = self.export_dir / 'plant_disease_model.onnx'\n",
    "            tf_model_path = str(self.export_dir / 'tensorflow_model')\n",
    "            if not Path(tf_model_path).exists():\n",
    "                print(\"TensorFlow SavedModel not found, attempting to create it for ONNX conversion.\")\n",
    "                saved_model_path = self.export_tensorflow_model()\n",
    "                if not saved_model_path:\n",
    "                    print(\"Skipping ONNX export as SavedModel creation failed.\")\n",
    "                    return None\n",
    "\n",
    "            import subprocess\n",
    "            command = [\n",
    "                \"python\", \"-m\", \"tf2onnx.convert\",\n",
    "                \"--saved-model\", tf_model_path,\n",
    "                \"--output\", str(onnx_path),\n",
    "                \"--opset\", \"13\"\n",
    "            ]\n",
    "            result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "            print(\"tf2onnx stdout:\", result.stdout)\n",
    "            print(\"tf2onnx stderr:\", result.stderr)\n",
    "\n",
    "            print(f\"‚úÖ ONNX model saved: {onnx_path}\")\n",
    "            return onnx_path\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to export ONNX model (subprocess error): {e}\")\n",
    "            print(\"stdout:\", e.stdout)\n",
    "            print(\"stderr:\", e.stderr)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to export ONNX model: {e}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_class_mapping(self):\n",
    "        print(\"üìã Creating class mapping files...\")\n",
    "        class_mapping = {\n",
    "            \"classes\": [\n",
    "                {\n",
    "                    \"id\": i,\n",
    "                    \"name\": class_name,\n",
    "                    \"display_name\": class_name.replace('_', ' ').title()\n",
    "                }\n",
    "                for i, class_name in enumerate(self.class_names)\n",
    "            ],\n",
    "            \"num_classes\": len(self.class_names),\n",
    "            \"model_info\": {\n",
    "                \"input_shape\": list(self.input_shape)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        json_path = self.export_dir / 'class_mapping.json'\n",
    "        with json_path.open('w') as f:\n",
    "            json.dump(class_mapping, f, indent=2)\n",
    "\n",
    "        txt_path = self.export_dir / 'class_names.txt'\n",
    "        with txt_path.open('w') as f:\n",
    "            f.write(\"\\n\".join(self.class_names))\n",
    "\n",
    "        print(\"‚úÖ Class mapping files created:\")\n",
    "        print(f\"  JSON: {json_path}\")\n",
    "        print(f\"  Text: {txt_path}\")\n",
    "        return json_path, txt_path\n",
    "\n",
    "    def create_deployment_config(self):\n",
    "        print(\"‚öôÔ∏è Creating deployment configuration...\")\n",
    "        config = {\n",
    "            \"model\": {\n",
    "                \"name\": \"Plant Disease Detection Model V2\",\n",
    "                \"version\": \"2.0.0\",\n",
    "                \"description\": \"Advanced EfficientNetV2-based plant disease detection\",\n",
    "                \"framework\": \"tensorflow\",\n",
    "                \"input_shape\": list(self.input_shape),\n",
    "            },\n",
    "            \"preprocessing\": {\n",
    "                \"resize\": list(self.input_shape[:2]),\n",
    "                \"normalize\": True,\n",
    "                \"mean\": [0.485, 0.456, 0.406],\n",
    "                \"std\": [0.229, 0.224, 0.225]\n",
    "            },\n",
    "            \"classes\": self.class_names,\n",
    "            \"deployment\": {\n",
    "                \"recommended_format\": \"tensorflow_savedmodel\",\n",
    "                \"mobile_format\": \"tflite\",\n",
    "                \"cross_platform_format\": \"onnx\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        config_path = self.export_dir / 'deployment_config.json'\n",
    "        with config_path.open('w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ Deployment config created: {config_path}\")\n",
    "        return config_path\n",
    "\n",
    "    def export_all(self):\n",
    "        print(\"\\nüöÄ Starting complete model export process...\\n\")\n",
    "        exported_files = {}\n",
    "\n",
    "        exported_files['tensorflow'] = self.export_tensorflow_model()\n",
    "        exported_files['keras_h5'], exported_files['keras_new'] = self.export_keras_model()\n",
    "        # Skipping TFLite and ONNX exports\n",
    "        # exported_files['tflite'] = self.export_tflite_model()\n",
    "        # exported_files['onnx'] = self.export_onnx_model()\n",
    "\n",
    "        exported_files['class_json'], exported_files['class_txt'] = self.create_class_mapping()\n",
    "        exported_files['config'] = self.create_deployment_config()\n",
    "\n",
    "        print(f\"\\nüéâ Model export completed! (Note: TFLite and ONNX exports were skipped)\\n\")\n",
    "        print(f\"üìÅ All available files saved to: {self.export_dir.resolve()}\")\n",
    "        return exported_files\n",
    "\n",
    "\n",
    "# --- Execute the export process ---\n",
    "# Initialize the ModelExporter\n",
    "exporter = ModelExporter(model, data_gen.class_names)\n",
    "\n",
    "# Start the export process\n",
    "exported_files_info = exporter.export_all()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
